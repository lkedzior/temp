Update this file when you spot something interesting while reading more verbose versions

##################Chapter 9 Trades and Quotes
 Minimize calls to each

 we need to sort each of the lists
 q)asc each genList each 3#5
 0 1 2 3 4
 0 1 2 3 4
 0 1 2 3 4

 we used 2 each to do that. We can use lambda or composition with @ to minimize calls to each

 1) Using lambda
 q){asc genList x} each 3#5

 2) Using composition
 q)(asc genList@) each 3#5



9.2 Quotes
Step Function - can be created with dictionaries by applying sort attribute `s# on a dictionary (not just on its key)

When we index into the dictionary/step function, q will perform a binary search
and return the result of the bin operation rather than '?' lookup.

The same applies to a keyed tables
We can see the `s# is applied in 3 places,
to the keyed table as a whole, in addition to the dictionary's key (a table) and also to the first column of the key
q)0N!kt:`s#([k]v)
`s#(`s#+(,`k)!,`s#0N 5 10 25 50 100)!+(,`v)!,0.01 0.05 0.1 0.25 0.5 1
k  | v
---| ----
   | 0.01
5  | 0.05
10 | 0.1
25 | 0.25
50 | 0.5
100| 1
q)

q)kt 15
v| 0.1
q)

############qsql
----by clause
select avg price by id from t

update ret:deltas[0n;log price] by id from t

The by clause performs three actions
1) groups the data using the columns in the by clause
2) Iterates over each group(distinct value) and applies our function
3) Final step depends on whether we used a select or update
select - will sort the results (returns keyed table)
update - does not return a keyed table, will align the results to match the location of the original layout

e.g. notice only one raw with id=0 in the select results
and multiple entries with id=0 in the update case (result of update by is not keyed table)

q)t
id date       price
----------------------
0  2000.01.01 100
0  2000.01.02 100.0632
0  2000.01.03 100.122
...

q)select avg price by id from t
id| price
--| --------
0 | 85.8896
1 | 111.7553
2 | 56.75649
3 | 119.7042
4 | 96.39676
q)

q)update ret:deltas[0n;log price] by id from t
id date       price    ret
------------------------------------
0  2000.01.01 100
0  2000.01.02 98.99257 -0.01012537
0  2000.01.03 99.00905 0.0001664493
0  2000.01.04 98.0201  -0.01003867
...

Update by with aggregate function
When an aggregate function is used,
it is applied to each group of values and the result is assigned to all records in the group.
        update av:avg p by n from t
n p  av
---------
a 10 11
b 15 14.5
a 12 11
c 20 22.5
c 25 22.5
b 14 14.5

################14.2 Pivot Tables
We are working with a table with date | id | price columns
q)"i"$.util.pivot 2!select date,id,price from t
date      | 0   1   2
----------| -----------
2000.01.01| 100 100 100
2000.01.02| 103 104 99
2000.01.03| 105 103 100
2000.01.04| 104 106 98

Q Tip 14.8 Only transform tables into pivot tables for presentation

Generating of a pivot table should be one of the last steps in any analysis. Adding an extra summary column and row at the end
and bottom of the table is probably the only adjustment we are likely to make to our pivot table.

When deciding which field to place in the column headers (date or id?)) - we should place the field with the fewest distinct items tin te column headers (so id not date)

Our pivot table takes single argument a keyed table
It derives all configuration parameters from the incoming table.

Steps involved when transforming to pivot
table with date | id | price columns

1) using exec by crete a dictionary
q)exec id!price by date from t
2000.01.01| 0 1 2!100 100 100i
2000.01.02| 0 1 2!103 104 99i
2000.01.03| 0 1 2!105 103 100i
2000.01.04| 0 1 2!104 106 98i

2) turn the key into a table by giving name to the key (standard exec by behaviour)
q)exec id!price by date:date from t
date      |
----------| ------------------
2000.01.01| 0 1 2!100 100 100i
2000.01.02| 0 1 2!103 104 99i
2000.01.03| 0 1 2!105 103 100i
2000.01.04| 0 1 2!104 106 98i

And to promote the list of dictionaries to a table we need to convert the keys to symbols
q)exec (`$string id)!price by date:date from t
date      | 0   1   2
----------| -----------
2000.01.01| 100 100 100
2000.01.02| 103 104 99
2000.01.03| 105 103 100
2000.01.04| 104 106 98

These are the basic operations
1) use exec by and give name to the key to get a table
2) make sure the pivot column names are symbol

This will not work if we are missing some values and dictionaries do not conform
e.g. when we remove id=0 for 2000.01.01
q)exec (`$string id)!price by date:date from 1_t
date      |
----------| -------------
2000.01.01| `1`2!100 100i
2000.01.02| `0`1`2!103 104 99i
2000.01.03| `0`1`2!105 103 100i

That's why we must endure each dictionary has the same number, and ordering, of elements.

We can do this by first constructing a unique list of ids and then use take operator on the dictionary so that
null is returned for missing cases

q)u:`$string distinct t`id
q)exec u#(`$string id)!price by date:date from 1_t
date      | 0   1   2
----------| -----------
2000.01.01|     100 100
2000.01.02| 103 104 99
2000.01.03| 105 103 100


############from the code kx
////Pivoting Pivot a table
t:([]k:1 2 3 2 3;p:`xx`yy`zz`xx`yy;v:10 20 30 40 50);
P:asc exec distinct p from t;
exec P#p!v by k:k from t



###############
select[5] from t /first 5 rows, works for memory and splayed tables only
select[-5] from t /last 5 rows
select[5 2] from t /returns 2 rows, 5 is the initial row number

USING THIS VERSION OF SELECT PREVENTS Q FROM HAVING TO ALLOCATE MEMORY FOR THE WHOLE RESULT SET,
ONLY FOR US TO TRUNCATE IT LATER


####Sorting
With xasc and xdesc we sort the table AFTER IT HAS BEEN CREATED. We don't have to do that.
Instead we can sort at the time of table selection (we just specify which comparison operation to use)
Using this technique is useful when the sorting criteria is not an existing column in the table

q)select[< abs price] from t   /sort by absolute price (ascending)

To sort by multiple columns
/sort descending by date but ascending by ID
q)select[>([]date;neg id)] from t

/we can combine the row selection and sorting
q)select[5 5;>([]date;neg id)] from t


####Functional version of the above
parse "select from t"
(?;`t;();0b;())
parse "select [5] from t"
(?;`t;();0b;();5j)
parse "select [5 5] from t"
(?;`t;();0b;();5j, 5j)
parse "select [< abs price] from t"
(?;`t;();0b;();0Wj;enlist (<:;(abs;`price)))
parse "select [5 5;< abs price] from t"
(?;`t;();0b;();5j, 5j;enlist (<:;(abs;`price)))

#######################fby
////	fby (filter by)   ////
/e.g. show all entries with price greater than average
/fby is part of the where filter...it filters out rows which don't meet the condition, it does not group the result set

/use it if you need to group by + use an aggregate function in the where phrase
q)select from t where price>(avg;price) fby ([]sym)
q)select from t where price>(avg;price) fby sym
select from t where price>(avg;price) fby ([]sym;ex)

How to select rows from a table with max price for each symbol?

q)t:([]sym:`aaa`aaa`aaa`bbb;date:.z.d;price:10 10 9 100)
q)t
sym date       price
--------------------
aaa 2018.05.07 10
aaa 2018.05.07 10
aaa 2018.05.07 9
bbb 2018.05.07 100

We could sort the table by price and select last row by sym

q)select by sym from `price xasc t
sym| date       price
---| ----------------
aaa| 2018.05.07 10
bbb| 2018.05.07 100
q)

but it is possible to ahieve the same much more efficiently without sorting with fby
q)select from t where 0=(rank;neg price) fby sym

The rank operator provides the ordinal ranking of a list where the smallest value receives a rank of 0
q)rank 1 1 2 2 3
0 1 2 3 4
q)rank 3 1 1 2 2
4 0 1 2 3

It is also possible to rank values/list between a min and max rank
e.g. max rank is 5

q)5 xrank til 10
0 0 1 1 2 2 3 3 4 4

q)select from t where 0=(rank;neg price) fby sym
sym date       price
--------------------
aaa 2018.05.07 10
bbb 2018.05.07 100
q)

Grouping by more than one column requires a table to be specified
select from t where 0=(rank;neg price) fby ([]sym;date.year)

###Asof Joins
aj[`id`time;t;select from quotes where date=dt]
The last specified join column must be the temporal column.
All asof joins use the binary search algorithm to find the proper rows to join.
For this to work properly, the temporal column must be sorted in ascending order.
All non-temporal join columns are used to find an exact match to narrow the rows before performing the binary search.
It is within the narrowed set of rows that the temporal column must be sorted - not the whole column.

aj returns the time from the left table
aj0 operator returns the time from the right table
To get both
aj[`id`time;t] select id, time, qtime:time, bp,ap from quotes

for performance We have to have `p or `g attribute on the first exact match column in the right table

######Performance of the aj
Pefrormance of the asof join is very sensitive to how fast the exact match can be performed.
Matching on more than two columns can slow the join down. After the first column is matched,
any attribute on the second column will be LOST, resulting in a linear search.

Instead of performing a 3-column asof join
t:select from trades
aj[`date`id`time;t] select from quotes

it is much more efficient to do it daily
(less memory used and attributes within each quote partition are preserved)
t:select from trades
g:t group t`date
q)t group t`date
2018.05.08| +`date`id`size`price!(,2018.05.08;,0;,10;,101)
2018.05.09| +`date`id`size`price!(,2018.05.09;,1;,20;,102)
2018.05.10| +`date`id`size`price!(,2018.05.10;,2;,30;,103)

raze {[dt;events]aj[`id`time;events] select from quotes where date=dt}'[key g;g]


###########wj join
There are two assumptions about the data that must be true.
Firstly, in addition to the time column, there can only be a single column which uniquely identifies a record.
And secondly, the table must be sorted first by the identifier
and then by time with the partition attribute `p on the identifier column.

Other joins allow the data to be un-partitioned as long as a `g attribute is placed on the identifier column.
THE wj OPERATOR, HOWEVER, DOES NOT ALLOW THIS.
The query will execute, but the results will be meaningless.

For this reason it is not possible to use the wj operator across multiple ids on a real-time database.
It is meant to be used on an historical database where data has been sorted within partitioned sections.

wj[t.time+/: -0D00:00:01 0D;`id`time;t](q;(avg;`bp);(avg;`ap)

wj considers prevailing quote
wj1 does not consider events outside the window

Due to its implementation, the wj operator can only work with integer temporal values.
This means that the datetime and any other float value can not be used with wj or wj1


####Joins for Dictionaries

aj and lj have variants that work for dictionaries as well.
Joining a dictionary with a table comes in handy when implementing CEP upd function. Single events
can arrive as dictionaries and will then need to be merged with existing data for analysis.

//lj using , example
//create a dictionary for this example
q)d:((1#`id)!1#0)
q)d
id| 0

q)ref:([id:0 1]; size:10 10; price: 100 101)
q)ref
id| size price
--| ----------
0 | 10   100
1 | 10   101
q)
q)d,ref   //note ref must be keyed table
id   | 0
size | 10
price| 100

The aj also has a corresponding operator that works for dictionaries and tables: asof.

q)quote:([]id:0 1; time:0D10; size:10 20; price:101 102)
q)quote
id time                 size price
----------------------------------
0  0D10:00:00.000000000 10   101
1  0D10:00:00.000000000 20   102
q)quote asof `id`time!(0;0D11)
size | 10
price| 101

//to recover the all fields we can save them and then prepend them to the result of asof
q)k,quote asof k:`id`time!(0;0D11)
id   | 0
time | 0D11:00:00.000000000
size | 10
price| 101
q)

###########Splayed tables (From Kdb for mortals)
Build in operations on the persisted splayed table
`:/db/t/ upsert (10:00:00;42.0)  //appending
`p xasc `:/db/t                  //sorting
@[`:/db/t;`p;`p#]                //applying attribute
@[`:/db/t2;enlist `s2;`:/db/sym?]  //re-enumerating a symbol column

NOTE By convention, all symbols are enumerated over a single domain named sym, although kdb+ does not require this,
the .Q utilities follow this convention so to be able to use them the good practice is to stick to domain named sym.

Query Execution on Splayed Tables

•	when kdb+ executes a query against a splayed table, it loads into memory only the columns that the query references.
Moreover, kdb+ only access the items of the target columns it actually needs (as per where clause)

####LINK COLUMNS

Foreign key column is an enumeration /ɪˌnjuːmə’reɪʃən/ over a keyed table.

Links are useful to implement FKs when dealing with non-keyed columns.
•	A table (keyed or non-keyed) can contain a link to itself in order to create a parent-child relationship
•	You can also use links to create “foreign key” between splayed tables where enumerated FKs are not possible (a keyed table cannot be splayed)

Link column is a list row numbers from the other table
We use ? to create a vector of indices of fid in id and we use ! to create the link

    t1:([] id: 1 2 3; p:1.1 2.2 3.3)
    t2:([] fid:2 1 3 2; v:20 10 30 40)
q)t1.id?t2.fid
1 0 2 1

//below (1 0 2 1) indeces refer to actual row index in the t1
q)update clink:`t1!(1 0 2 1) from `t2  //WE REFERENCE t1 TABLE when creating a link
`t2
q)t2
fid v  clink
------------
2   20 1
1   10 0
3   30 2
2   40 1
q)meta t2
c    | t f  a
-----| ------
fid  | i
v    | i
clink| i t1

//reminder what we were doing for Foreign key to A KEYED table
//use enumeration to create FK column when defining a table
//fkcolumn:`keyedTable$val1 val2 val3    /column is an enumeration, keyedTable is a domain

#### Partition tables
Examples of Partition Domains
Possible domains are: date, month, year, int

Working with partitioned tables
count, cols, meta – these work as expected

The select template, or the equivalent function form, is the way to access data for a partitioned table.
The other templates (exec, update, delete) do not work on partitioned tables.





//memory usage of the below query
//normally partition is just memory mapped and should not increase the RES memory usage
//but because we select all columns kdb will generate date column vector of the size of the t table for that date
//this can increase memory usage depending on the size
count select from t where date=dt

Map reduce example
Simple example with aggregations like max, count, sum
•	Map: Split the list, hand on sub-lists to each resource and ask for aggregation number e.g. max
•	Reduce: Collect the results and calculate the maximum of the individual maximums

More complex example with aggregation like average
•	Map: Split the list, hand on sub-lists to each resource and ask for a sum and a count
•	Reduce: obtain the overall average (sum partial sums) / (sum partial counts)

More generally on map-reduce
map-reduce decomposes an operation/problem on a list into two sub-operations
•	map operation – performed on each sublist to obtain a list of partial results
•	reduce operation – the partial result lists are combined to obtain the final result

The aggregates that q can decompose with map-reduce
avg, count, first, last, max, min,sum, and also
X cor Y 	//correlation
X cov Y 	//covariance
dev X		//standard deviation function
prd X		//product function
var X		//variance
X wavg Y	//weighted average: (sum X*Y)%Y
X wsum Y	//weighted sum: sum X*Y



//The naive way to perform an asof join loads all requisite data at once.
aj[`date`sym`ti;select from t where date within dr;
		select from q where date within dr]

//as mentioned aj does not perform map-reduce optimization
//here we show how we can use slaves and peach
//to take advantage of parallel I/O and concurrent processing
    >q -s 2
aj1:{[d] aj[`sym`ti; select from t where date=d; select from q where date=d]}
raze aj1 peach 2009.01.01 2009.01.02

Query execution on segmented tables (par.txt segmentation)
Steps
1.	Kdb+ compares the query’s requisite partitions to the segment layout in par.txt
2.	The result is a nested list, each item being the partitions list for one segment
3.	To execute the map step, kdb+ creates a revised query containing the map sub-operation from original query. Example: revised query for avg is ‘compute the sum and count of the sublist’
4.	Revised query is dispatched to all n slaves via peach. Query is peached over a list of partitions from point 2. Each slave is provided with the list of partitions for one segment and is directed to compute the revised query for its segment.
NOTE: Kdb+ will only use as many slaves to process a query as there are segments in the query footprint. So if no par.txt is available the slaves threads will NOT be used for the map step
Vanilla partitioned table (i.e. without a par.txt) is treated as a single segment

5.	within one slave the revised query is applied against a segments’ partition footprint…query is sequentially applied across partitions and the result tables are returned as a list of tables representing one segment result
6.	Once all slaves complete the nested list of segment results is flattened and reordered by partition value.
7.	Finally kdb+ employs the original query reduce step to combine the full list of ordered partition results into the query result table

KDB database
What happens at startup?

When you point q at a directory or reload with \l it does the following things in this specific order
•	loads q entities (lists, dict, tables, functions)
•	maps splayed tables
•	maps partitioned or segmented tables (only valid partitioned tables are mapped)
•	executes scripts (as this is the last step, you can use loaded entity from there)

Note: Anything that q does not recognize causes it to abort that portion of startup. E.g. if q discovers unexpected files in partitioned or segmented directory, it will not map the tables contained therein. Watch out unexpected hidden files

Note Q will not automatically load serialized data whose file names have extensions.


.Q utilities

.Q.chk `:/db – this utility will examine each partition subdirectory in the root /db and writes an empty splayed table of the appropriate form where necessary
.Q.ind Utility to access rows in a partitioned table based on an absolute index.
.Q.en[`:/db;table]
.Q.dpft		//[directoryRoot;partition;`p#field;tableName]
.Q.dpfts allows the enum domain to be specified. Since V3.6 (2018.04.13)
.Q.dpfts[`:db;2007.07.23;`sym;`t;`mysym]
`t
q)mysym
`c`a`b

.Q.hdpf (save tables)
Syntax: .Q.hdpf[historicalport;directory;partition;`p#field]

.Q.par[`:.;2010.02.02;`quote]
`:/data/taq/2010.02.02/quote

.Q.fs [f;textFileHandle] Loops over a text file and reads it in “chunks” (each chunk is a list of complete records – “\n" delimited) and allows you to apply a function to each list.
.Q.fsn is almost identical to .Q.fs but it takes an extra argument: z is the size in bytes that chunks will be read in. This is particularly useful for balancing load speed and ram usage.
.Q.fs is in fact a projection of .Q.fsn with the size set to 131000 bytes by default.

.Q.fu – function unique, see implementation of this one
.Q.fc (parallel on cut) .Q.fc[f]vec returns the result of evaluating f vec


vwap
There is built-in wavg(Weighted average function)
The calculation is (sum x*y) % sum x
The financial analytic known as VWAP (volume-weighted average price) is a weighted average.
q)select size wavg price by sym from trade
sym| price
---| -----
a  | 10.75

In a CEP typically we calculate wsum[size;price] & sum size (and keep adding new values)
if[x~"vwap";t:`trade;
 upd:{[t;x]vwap+:select price:size wsum price,size:sum size by sym from x}]

and vwap can be calculated on request as price%size

standard deviation
var, svar (variance and sample variance (svar(x) = (n%n-1)*var(x)
dev, sdev - standard deviation as the square root of the variance
q)dev 10 343 232 55
134.3484
q)select dev price by sym from trade

A useful property of the standard deviation is that, unlike the variance,
it is expressed in the same units as the data.

see
https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/variance-standard-deviation-sample/a/population-and-sample-standard-deviation-review
Population and sample standard deviation
Standard deviation measures the spread of a data distribution. It measures the typical distance between each data point and the mean.
The formula we use for standard deviation depends on whether the data is being considered a population of its own, or the data is a sample representing a larger population.
If the data is being considered a population on its own, we divide by the number of data points, N.
If the data is a sample from a larger population, we divide by one fewer than the number of data points in the sample, n-1.

The steps in each formula are all the same except for one—we divide by one less than the number of data points when dealing with sample data.
Step 1: Calculate the mean of the data—this is \muμmu in the formula.
Step 2: Subtract the mean from each data point. These differences are called deviations. Data points below the mean will have negative deviations, and data points above the mean will have positive deviations.
Step 3: Square each deviation to make it positive.
Step 4: Add the squared deviations together.
Step 5: Divide the sum by the number of data points in the population. The result is called the variance.
Step 6: Take the square root of the variance to get the standard deviation.

##########Why and when vectors are good


###########hash table implementation



############notes from kdb for mortals
//flushing buffer after asynch call
(neg h)[]
async send, e.g. neg[h] data
merely queues the data for sending later. The actual writing into sockets begins at any of the following
1)when the current/pending queries/script complete
2)a sync request is issued on h
3)an async purge on h is performed, i.e. neg[h][]; / blocks until all pending msgs on h are sent

####
Qtips Memory allocation (chapter 10)
When more memory is needed than exists in the pool/heap, kdb+ will allocate additional multiples of 64MB blocks.
e.g in 3.5 we see increases by 64MB each time

When the process memory limit has been reached, q will coalesce /kəʊ.əˈles/ blocks of unused memory available in the pool/heap
that are at least 64MB, and return them back to operating system. Kdb+ then tries to allocate the requested block of memory,
if is still unable to allocate it, a `wsfull error is thrown and the process dies.

- it is possible to request memory to be coalesced by using the .Q.gc[]
-in addition q can be configured to automatically release blocks of memory that are at least 64MB by setting the -g 1

/using null item :: to define general list
L:(1;2;3;`a;::)
L[3]:4	   /would change L type 0h->6h without :: at the end
L[3]:`a    /and here would fail without :: at the end


/function projection is the way to DEFINE new functions
/by using existing one and default argument

. and @    /apply for functions

f[1;2] ~ f . 1 2 ~ .[f;1 2] ~ .[`f;1 2]	 /right argument must be a list e.g. f:{x*x} f . enlist 2
f[1] ~ f @ 1 ~ @[f;1] ~ @[`f;1]

/. and @ can be used to alter list/dict/table by applying a function f
/to modify original list/dict/table refer by name e.g. `L

/L@I - indexing at the top level
/L.I - indexing at depth
@[L;I;f]    /apply monadic f
@[L;0 1;neg]	 /will negate items at index 0 and 1 and return new List
@[`L;0 1;neg]	 /will modify L and return `L

@[L;I;f;y]  /apply dyadic f
@[L;1 2; +; 42 43]	 /add 42 to L[1], add 43 to L[2]


/traditional assign fails at depth level
q)L[0][1]:42	 /would work at top level e.g. L[0]:(1 42 3)
'assign
/use indexing at depth instead
q).[L;0 1; : ;42]


//Adverbs (from Adverbs.q)
////    ADVERBS for monadic functions    ////
f each L	 /iterate over List and apply funcion f

/scan can be used for 3 different cases to execute recursive calls
/ 1) n recursive calls, return a list with consecutive results
f\[n;arg]	 /n recursive calls, returns the list with consecutive results

/ 2) do while(cond) - recursive calls as long as condition is true (stops after first false)
fMonad\[monadicCondFunction;100]  //recursive calls as long as monadicCondFunction returns 1b

/ 3) recurive calls until argument = result (converge)
f\[arg]

////    ADVERBS for dyadic functions    ////
easy way to remember syntax: it always starts with f/ or f\ or f'
f/ f\ f/: f\: f' f':
f\:[L;a]	 /each left		L f\: a
f/:[a;L]	 /each right	a f/: L

f'[L1;L2]	 /each both
f'[a;L]		 /a expanded to list
f'[L;a]		 /a expanded to list

f':[a;L]	 /each previous, applies f on consecutive items in a List

/over and scan are different, they use previous results for next call
/generally f/ ~ last f\
f/			 /over
f\			 /scan

##############/scan & over for multivalent functions

##########scan with triadic function
q)f:{show -3!(x;y;z); x+y+z}
q)(f\)[1;10 20 30; 100 200 300]  // (f\)[arg;L1;L2]  //will run operation doing each both on L1&L2   f[prevResult;L1;L2]
"1 10 100"  //111
"111 20 200"  //331
"331 30 300"  //661
111 331 661
q)

//// combining adverbs ////
combining adverbs
in general f adverb1 advert2
  => (f adverb1) adverb2



####dictionaries
/reverse lookup with ? and where
q)(`a`b`c!1 2 2)?2  //find first occurance
`b
q)2=(`a`b`c!1 2 2)
a| 0
b| 1
c| 1
q)where 2=(`a`b`c!1 2 2)  //find all occurances
`b`c


###keyed tables
Inserting data into a keyed table works just like inserting data into a regular table,
with the additional requirement that the key must not already exist in the table.

/get multiple rows using list of keys
q)kt[flip enlist 1 3]	or	kt[(enlist 1; enlist 3)]
c1 c2
-----
a  10
c  30

/Using simple lookup, this is more efficient than above flip (it's costrly to transpose list)
q)kt[([]eid:1 2)]
c1 c2
-----
a  10
b  20

/Using take operator
q)([]eid: 1 3)#kt
eid| c1 c2
---| -----
1  | a  10
3  | c  30

/ xkey - convert non-keyed <-> keyed table
() xkey `kt    /convert to non-keyed table, same as 0! kt
`eid xkey `kt  /convert to keyed table


Foreign key columns
////	Foreign Key    ////
/use enumeration to create FK column when defining a table
fkcolumn:`keyedTable$val1 val2 val3    /column is an enumeration, keyedTable is an domain

grouping ungrouping
/xgroup (don't confuse with ungroup)
xgroup – another way to group the table results. (instead of select by)
xgroup always shows all columns in the results
`byColumn xgroup table
q)`b xgroup ([]a:0 0 1 1 2;b:`a`a`c`d`e;c:til 5)
b| a   c
-| -------
a| 0 0 0 1
c| ,1  ,2
d| ,1  ,3
e| ,2  ,4
q)


ungroup t	 /normalize results to flat table
q)t		 /note c3 column is not simple list
c1 c2 c3
---------
a  10 1 2
b  20 3 4

/////    Functional forms of select/exec/update/delete    ////
/better than prametrized queries because column can be dynamic

?[t;c;b;a]	 /select, exec
![t;c;b;a]	 /update
![t;c;0b;a]	 /delete

t- table
c- list of contraints
b- dictionary of group bys (it is list for exec)
a- dictionary of aggregates

  where p>0, n in `x`y	 	 /c: ((>;`p;0);(in;`n;enlist `x`y))  //note symbol literals are enlisted, column names are not


///////////		Reading CSV files	///////////////
//nested list is returned by 0:
flip `id`sym`px!("ISF"; ",") 0: `:/data/Px.csv

//delimited text file with columns as first line
//here 0: returns a table
("ISF";enlist ",") 0: `:/data/pxtitles.csv
Seq  Sym         Px
-----------------------
1001 DBT12345678 98.6
1002 EQT98765432 24.75
1004 CCR00000001 121.23

/
functions, global variables, context
If a function defines global variable, the context of this variable
depends on the context in which the function had been defined

example from tick

\d .u
init:{w::123}
\d .
.u.init[]
.u.w ->123 w is now in u context
\


//JOINS
column join
/ ,' column join to create new table, requires same count
/t1,'t2 joins records to records
t1 ,' t2

//another way to do it is with ^
t1^t2

/////////////////////////////////    upsert , ^ uj    /////////////////////////////

/t1 upsert t2 or t1,t2	- upsert or insert if t1 is non keyed
/t1 uj t2 is like upsert but works for different table shapes
/kt1 ^ kt2 is special version of upsert for keyed tables, which does not override with nulls

/upsert works only if both tables have got the same columns
/if first table is keyed table then upsert is performed
/otherwise insert is performed
/you can update first table by passing reference
/t1,t2 does same thing as upsert
t1 upsert t2
kt1 upsert kt2
kt1 upsert t2
/t1 upsert kt2 - does not work

/////////////////////////////////    uj union join    /////////////////////////////

t1 uj t2	or 		kt1 uj	kt2

/similar to upsert but columns don't have to match exactly
/1)when used with unkeyed tables - the second table is inserted
/2)when used with keyed tables (both must be keyed) the upsert is performeds



KEYED JOINS
Some joins are keyed, in that columns in the first table are matched with the key columns of the second table
Second table must be keyed
t1 join kt2
kt1 join kt2
/FK relation will be used if exists
/if there is no FK column in leftTable then column names must match PK columns

ij - inner join
lj - left join (also ljf variation)
pj - plus join, variation of left join, t2 values are added to t1 for common columns


////    equijoin ej[c;t1;t2]  /'ikwi, 'ɛkwi/  ////
/ej verb joins two tables on a given list of columns c
/ej is like keyed inner join (ij) but it works with non-keyed tables

/what happens when second table has duplicates?
/All combinations will be included in the result table
q)ej[`sym;s;t]
sym  ex  MC   price
-----------------------
IBM  N   1000 0.7029677
IBM  N   1000 0.2608152
MSFT CME 250  0.5433888